# ğŸ“ KTU AI Assistant

A modern, full-stack Retrieval-Augmented Generation (RAG) system for Kerala Technological University (KTU) academic queries. This intelligent assistant helps students get accurate answers about syllabus, notes, rules, and academic guidelines using uploaded PDF documents.

## ğŸ—ï¸ Architecture

```
KTU AI Assistant
â”œâ”€â”€ ğŸ”§ Backend (FastAPI + LangChain + Ollama)
â”œâ”€â”€ ğŸ¨ Frontend (React + TypeScript + Tailwind)
â”œâ”€â”€ ğŸ¤– AI Model (Llama3 via Ollama)
â””â”€â”€ ğŸ“Š Vector DB (ChromaDB)
```

## ï¿½ Project Structure

```
KTU_AI_Assistant/
â”œâ”€â”€ ğŸ“‚ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ ğŸ“‚ app/                    # Application logic
â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI app & routes
â”‚   â”‚   â”œâ”€â”€ rag_service.py         # RAG implementation
â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”‚   â””â”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ ğŸ“‚ data/                   # Data storage
â”‚   â”‚   â”œâ”€â”€ docs/                  # PDF documents
â”‚   â”‚   â””â”€â”€ vector_db/             # ChromaDB storage
â”‚   â”œâ”€â”€ ğŸ“‚ static/                 # Static files
â”‚   â”œâ”€â”€ ğŸ“‚ venv/                   # Python virtual environment
â”‚   â”œâ”€â”€ server.py                  # Main server entry point
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“‚ frontend/                   # React Frontend
â”‚   â”œâ”€â”€ ğŸ“‚ src/                    # Source code
â”‚   â”‚   â”œâ”€â”€ App.tsx                # Main React component
â”‚   â”‚   â”œâ”€â”€ main.tsx               # Entry point
â”‚   â”‚   â””â”€â”€ index.css              # Tailwind styles
â”‚   â”œâ”€â”€ ğŸ“‚ public/                 # Public assets
â”‚   â”œâ”€â”€ package.json               # Node.js dependencies
â”‚   â”œâ”€â”€ tailwind.config.js         # Tailwind configuration
â”‚   â””â”€â”€ vite.config.ts             # Vite configuration
â”œâ”€â”€ ğŸ“‚ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ setup.bat                  # Initial setup script
â”‚   â””â”€â”€ start_servers.bat          # Start both servers
â”œâ”€â”€ ğŸ“„ README.md                   # This file
â”œâ”€â”€ ğŸ“„ .env                        # Environment variables
â””â”€â”€ ğŸ“„ .gitignore                  # Git ignore rules
```

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+** with pip
- **Node.js 20.19.0+** with npm
- **Ollama** installed and running
- **Git** (optional, for cloning)

### ğŸ”§ One-Command Setup

```bash
# Clone the repository (if needed)
git clone <repository-url>
cd KTU_AI_Assistant

# Run the complete setup
scripts\setup.bat
```

### ğŸ¯ Start the Application

```bash
# Start both backend and frontend servers
scripts\start_servers.bat
```

**That's it!** ğŸ‰ Your KTU AI Assistant will be running at:
- ğŸŒ **Frontend**: http://localhost:5173
- âš™ï¸ **Backend**: http://localhost:8000
- ğŸ“– **API Docs**: http://localhost:8000/docs

## ğŸ’¡ Manual Setup (Advanced)

### Backend Setup

1. **Navigate to backend directory**:
   ```bash
   cd backend
   ```

2. **Create and activate virtual environment**:
   ```bash
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Start the backend server**:
   ```bash
   python server.py
   ```

### Frontend Setup

1. **Navigate to frontend directory**:
   ```bash
   cd frontend
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Start development server**:
   ```bash
   npm run dev
   ```

### Ollama Setup

1. **Download and install** from https://ollama.com/download
2. **Pull the Llama3 model**:
   ```bash
   ollama pull llama3
   ```
3. **Start Ollama** (usually starts automatically)

## ï¿½ Usage Examples

Once both servers are running, you can ask questions like:

- ğŸ’¼ **Course Information**: "What are the requirements for CST306?"
- ğŸ“‹ **Exam Patterns**: "What is the exam pattern for Algorithm Analysis?"
- ğŸ“– **Syllabus Details**: "Explain the syllabus for Data Structures"
- ğŸ“ **University Rules**: "What are the attendance requirements?"
- ğŸ¯ **Learning Outcomes**: "What will I learn in this course?"

## ğŸ”§ Features

### ğŸ”¥ Backend Features
- âœ… **RESTful API** with FastAPI
- âœ… **Document Processing** (PDF â†’ chunks â†’ embeddings)
- âœ… **Vector Search** with ChromaDB
- âœ… **AI Integration** with Ollama/Llama3
- âœ… **Async Processing** for better performance
- âœ… **Health Checks** and error handling
- âœ… **CORS Support** for frontend communication
- âœ… **Auto-documentation** with Swagger UI

### ğŸ¨ Frontend Features
- âœ… **Modern React 18** with TypeScript
- âœ… **Responsive Design** with Tailwind CSS
- âœ… **Real-time Queries** with loading states
- âœ… **Error Handling** with user-friendly messages
- âœ… **Keyboard Shortcuts** (Ctrl+Enter to submit)
- âœ… **Professional UI** with gradient backgrounds
- âœ… **Accessibility** features built-in

## ğŸ› ï¸ Development

### Adding New Documents
1. Place PDF files in `backend/data/docs/`
2. Restart the backend server
3. Documents will be automatically processed

### Configuration
Edit `backend/app/config.py` to customize:
- Ollama model and URL
- Chunk size and overlap
- File paths
- CORS origins

### Environment Variables
Create a `.env` file in the root directory:
```env
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3
DOCS_PATH=./backend/data/docs
VECTOR_DB_PATH=./backend/data/vector_db
```

## ğŸš¨ Troubleshooting

### Common Issues

**âŒ Backend fails to start**
- âœ… Ensure Python virtual environment is activated
- âœ… Check if all dependencies are installed
- âœ… Verify Ollama is running

**âŒ Frontend shows CORS errors**
- âœ… Ensure backend is running on port 8000
- âœ… Check CORS configuration in `config.py`

**âŒ No AI responses**
- âœ… Verify Ollama is running: `ollama list`
- âœ… Check if Llama3 model is installed: `ollama pull llama3`
- âœ… Ensure documents are in `backend/data/docs/`

**âŒ Slow responses**
- âœ… Local LLM inference can be slow
- âœ… Consider using a GPU for faster processing
- âœ… Reduce chunk size in configuration

### Getting Help

1. **Check the logs** in the terminal windows
2. **Visit API docs** at http://localhost:8000/docs
3. **Verify health** at http://localhost:8000/health
4. **Check GitHub issues** for known problems

## ğŸ¯ Performance Tips

- **ğŸ“Š Hardware**: 8GB+ RAM recommended for smooth operation
- **âš¡ GPU**: NVIDIA GPU significantly speeds up inference
- **ğŸ“ Documents**: Keep PDFs under 50MB for faster processing
- **ğŸ”„ Chunks**: Adjust chunk size based on document complexity

## ï¿½ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with a clear description

## ğŸ™ Acknowledgments

- **LangChain** for the RAG framework
- **Ollama** for local LLM deployment
- **FastAPI** for the modern Python API
- **React** for the frontend framework
- **ChromaDB** for vector storage

---

**ğŸ“ Built for KTU Students, by KTU Students** 

*Happy learning with your intelligent AI assistant!* ğŸ“–âœ¨
